# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

import torch
from torch import nn
from torch.nn import functional as F
from typing import List, Tuple, Type
from .common import LayerNorm2d

#
class MaskDecoder(nn.Module):
    def __init__(
        self,
        *,
        transformer_dim: int,
        transformer: nn.Module,
        num_classes: int = 3,
        activation: Type[nn.Module] = nn.GELU,
        # ğŸ‘‡ è¿™äº›å‚æ•°å…¶å®æ²¡ç”¨äº†ï¼Œä½†ä¿ç•™ç€é˜²æ­¢æŠ¥é”™
        iou_head_depth: int = 3,
        iou_head_hidden_dim: int = 256,
    ) -> None:
        """
        æ”¹é€ åçš„ Decoderï¼šä¸“æ³¨äº Multi-Class Semantic Segmentation
        ä¸å†é¢„æµ‹ IoUï¼Œä¸å†è¿›è¡Œæ­§ä¹‰æ€§é€‰æ‹©ã€‚
        """
        super().__init__()
        self.transformer_dim = transformer_dim
        self.transformer = transformer

        self.num_classes = num_classes

        self.num_mask_tokens = num_classes

        # 1. ã€æ‰‹æœ¯ã€‘ç§»é™¤ IoU Token
        # self.iou_token = nn.Embedding(1, transformer_dim) <--- åˆ æ‰å®ƒï¼
        
        # 2. ã€é‡å®šä¹‰ã€‘è¿™é‡Œçš„ mask_tokens ç°åœ¨å°±æ˜¯â€œç±»åˆ«é”šç‚¹â€ (Class Anchors)
        # Token[0] -> è´Ÿè´£æ‰¾èƒŒæ™¯
        # Token[1] -> è´Ÿè´£æ‰¾å·¦å¿ƒå®¤
        # Token[2] -> è´Ÿè´£æ‰¾å¿ƒè‚Œ
        self.class_embeddings = nn.Embedding(self.num_classes, transformer_dim)

        # 3. å›¾åƒç‰¹å¾ä¸Šé‡‡æ ·å±‚ (ä¿ç•™åŸæ ·)
        self.output_upscaling = nn.Sequential(
            nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2),
            LayerNorm2d(transformer_dim // 4),
            activation(),
            nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2),
            activation(),
        )

        # 4. æ¯ä¸ªç±»åˆ«ç‹¬ç«‹çš„ MLP (ä¿ç•™åŸæ ·)
        self.output_hypernetworks_mlps = nn.ModuleList(
            [
                MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3)
                for i in range(self.num_classes)
            ]
        )

        # 5. ã€æ‰‹æœ¯ã€‘ç§»é™¤ IoU é¢„æµ‹å¤´
        # self.iou_prediction_head = ... <--- åˆ æ‰å®ƒï¼

    def forward(
        self,
        image_embeddings: torch.Tensor,
        image_pe: torch.Tensor,
        sparse_prompt_embeddings: torch.Tensor,
        dense_prompt_embeddings: torch.Tensor,
        multimask_output: bool = True,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        
        # ç›´æ¥è°ƒç”¨é¢„æµ‹é€»è¾‘
        masks = self.predict_masks(
            image_embeddings=image_embeddings,
            image_pe=image_pe,
            sparse_prompt_embeddings=sparse_prompt_embeddings,
            dense_prompt_embeddings=dense_prompt_embeddings,
        )
        
        # ä¸ºäº†å…¼å®¹ sam.py çš„æ¥å£ (å®ƒæœŸæœ›è¿”å›ä¸¤ä¸ªå€¼)ï¼Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªå‡çš„ IoU
        # å½¢çŠ¶ [B, num_classes]
        batch_size = masks.shape[0]
        dummy_iou = torch.ones(batch_size, self.num_classes, dtype=masks.dtype, device=masks.device)
        
        return masks, dummy_iou

    def predict_masks(
        self,
        image_embeddings: torch.Tensor,
        image_pe: torch.Tensor,
        sparse_prompt_embeddings: torch.Tensor,
        dense_prompt_embeddings: torch.Tensor,
    ) -> torch.Tensor:
        
        # 1. ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä¸å†æ‹¼æ¥ IoU Token
        # output_tokens å°±æ˜¯æˆ‘ä»¬çš„ 3 ä¸ªç±»åˆ«æŸ¥è¯¢å‘é‡
        output_tokens = self.class_embeddings.weight
        
        # æ‰©å±•åˆ° Batch ç»´åº¦ [B, 3, 256]
        output_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1)
        
        # æ‹¼æ¥æç¤ºè¯ (BBox) -> [B, 3 + N, 256]
        tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)

        # 2. è¿è¡Œ Transformer
        # å®ƒç°åœ¨çš„ä»»åŠ¡æ˜¯ï¼šç»“åˆ BBox çš„ä½ç½®ä¿¡æ¯ï¼Œå»å›¾åƒé‡Œå¯»æ‰¾ 3 ç§ç‰¹å®šçš„ç‰¹å¾
        hs, src = self.transformer(image_embeddings, image_pe, tokens)
        
        # 3. æå–è¾“å‡º
        # hs çš„å‰ 3 ä¸ª token å°±æ˜¯æˆ‘ä»¬è¦çš„ç±»åˆ«ç‰¹å¾
        # è¿™é‡Œçš„ embedding ä»£è¡¨äº†æ¨¡å‹å¯¹ "èƒŒæ™¯"ã€"LV"ã€"Myo" çš„ç†è§£
        class_tokens_out = hs[:, 0 : self.num_classes, :]

        # 4. ä¸Šé‡‡æ ·å›¾åƒç‰¹å¾ (Pixel Features)
        b, c, h, w = image_embeddings.shape
        src = src.transpose(1, 2).view(b, c, h, w)
        src = src + dense_prompt_embeddings
        upscaled_embedding = self.output_upscaling(src)

        # 5. ç”Ÿæˆ Mask
        # æ¯ä¸ªç±»åˆ«ç”¨è‡ªå·±çš„ MLP ç”Ÿæˆä¸€ä¸ªæƒé‡å‘é‡ï¼Œå»å’Œå›¾åƒç‰¹å¾åšç‚¹ç§¯
        hyper_in_list = []
        for i in range(self.num_classes):
            hyper_in_list.append(self.output_hypernetworks_mlps[i](class_tokens_out[:, i, :]))
        hyper_in = torch.stack(hyper_in_list, dim=1)

        b, c, h, w = upscaled_embedding.shape
        masks = (hyper_in @ upscaled_embedding.view(b, c, -1)).view(b, -1, h, w)

        return masks

class MLP(nn.Module):
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        num_layers: int,
        sigmoid_output: bool = False,
    ) -> None:
        super().__init__()
        self.num_layers = num_layers
        h = [hidden_dim] * (num_layers - 1)
        self.layers = nn.ModuleList(
            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])
        )
        self.sigmoid_output = sigmoid_output

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)
        if self.sigmoid_output:
            x = torch.sigmoid(x)
        return x
