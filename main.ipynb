{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30136340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import nibabel as nb\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# sys.path.append('/host/d/Github')\n",
    "# import Whole_heart_segmentation_junzhe.functions_collection as ff\n",
    "# import Whole_heart_segmentation_junzhe.Data_processing as Data_processing\n",
    "# import Whole_heart_segmentation_junzhe.Build_lists.Build_list as Build_list\n",
    "# import Whole_heart_segmentation_junzhe.data_loader.random_aug as random_aug\n",
    "# import Whole_heart_segmentation_junzhe.data_loader.generator as generator\n",
    "# import Whole_heart_segmentation_junzhe.segment_anything.model as model\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from torch.cuda.amp import GradScaler\n",
    "# from tqdm import tqdm  # 进度条\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# 获取项目根目录（自动适配你的路径）\n",
    "root_dir = os.getcwd()  # 因为main.ipynb就在根目录里，所以当前目录就是根目录\n",
    "sys.path.append(root_dir)  # 把根目录加入Python的搜索路径\n",
    "\n",
    "# 后面的导入可以简化了\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nb\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 简化导入（不用写全路径了）\n",
    "import functions_collection as ff\n",
    "import Data_processing\n",
    "from Build_lists.Build_list import Build  # 精准导入Build_list.py里的Build类\n",
    "import data_loader.random_aug as randaug\n",
    "import data_loader.generator as generator\n",
    "import segment_anything.model as model  # 直接从segment_anything导入model\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler\n",
    "from tqdm import tqdm  # 进度条"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21de6f2",
   "metadata": {},
   "source": [
    "### step 1: define trial name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8cb9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = 'trial'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0354b4",
   "metadata": {},
   "source": [
    "### step 2: build patient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ad3f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all train img files: ['/host/d/GitHub/Whole_heart_segmentation_junzhe/example_data/data/ID_0002/img/slice_0.nii.gz'\n",
      " '/host/d/GitHub/Whole_heart_segmentation_junzhe/example_data/data/ID_0002/img/slice_1.nii.gz'\n",
      " '/host/d/GitHub/Whole_heart_segmentation_junzhe/example_data/data/ID_0002/img/slice_2.nii.gz'\n",
      " '/host/d/GitHub/Whole_heart_segmentation_junzhe/example_data/data/ID_0002/img/slice_3.nii.gz'\n",
      " '/host/d/GitHub/Whole_heart_segmentation_junzhe/example_data/data/ID_0002/img/slice_4.nii.gz'\n",
      " '/host/d/GitHub/Whole_heart_segmentation_junzhe/example_data/data/ID_0002/img/slice_5.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "# change the excel path to your own path\n",
    "patient_list_spreadsheet = os.path.join('/host/d/Github/Whole_heart_segmentation_junzhe/example_data/Patient_list','patient_list.xlsx')\n",
    "build_sheet = Build(patient_list_spreadsheet)\n",
    "# build_sheet =  Build_lists.Build(patient_list_spreadsheet)\n",
    "# train\n",
    "_, patient_id_list_train, slice_index_list_train, img_file_list_train, seg_file_list_train = build_sheet.__build__(batch_list = [0])  # just as an example, use batch 0 for train\n",
    "print('all train img files:', img_file_list_train)\n",
    "\n",
    "# # define val\n",
    "# _,_,input_file_val, reference_file_val = build_sheet.__build__(batch_list = [1])  # just as an example, use the same batch for val\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e644e31",
   "metadata": {},
   "source": [
    "### data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4def2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define this generator\n",
    "generator_train = generator.Dataset_CMR(\n",
    "    image_file_list = img_file_list_train,\n",
    "    \n",
    "    seg_file_list = seg_file_list_train,\n",
    "\n",
    "    image_shape = [128,128],\n",
    "    center_crop_according_to_which_class  = [1], #default: crop according to class 1 (LV)\n",
    "\n",
    "    shuffle = True,\n",
    "    image_normalization = True,\n",
    "    augment = True,\n",
    "    augment_frequency = 0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0108c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = generator_train\n",
    "dl = DataLoader(ds, batch_size = 1, shuffle = False, pin_memory = True, num_workers = 0)# cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8df27c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e0691bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功加载匹配的权重，跳过了形状不符的输出层。\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def get_args_parser( vit_type = \"vit_h\", original_SAM_weights = None):\n",
    "    parser = argparse.ArgumentParser('SAM fine-tuning', add_help=True)\n",
    "\n",
    "    parser.add_argument('--resume', default = original_SAM_weights)\n",
    "\n",
    "    parser.add_argument('--img_size', default=128, type=int) \n",
    "\n",
    "    parser.add_argument('--vit_type', default=vit_type, type=str, choices=[ 'vit_b', 'vit_l', 'vit_h'],)\n",
    "    \n",
    " \n",
    "    return parser\n",
    "\n",
    "original_sam = os.path.join('/host/d/Data/SAM_weights','sam_vit_b_01ec64.pth')\n",
    "\n",
    "args = get_args_parser(vit_type = \"vit_b\",original_SAM_weights = original_sam)\n",
    "args = args.parse_args([])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "our_model = model.build_model(args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc21e9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loaded image\n",
      "after augmentation, image min: 0.0  max: 1.0\n",
      "x shape:  torch.Size([3, 1024, 1024])\n",
      "input images:  torch.Size([1, 3, 1024, 1024])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# change batch_data[\"image\"] shape from (1, C, H, W) to (C,H,W)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m batch_data[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m batch_data[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m output \u001b[39m=\u001b[39m our_model([batch_data], multimask_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39moutput masks shape:\u001b[39m\u001b[39m'\u001b[39m, output[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmasks\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     14\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/host/d/GitHub/Whole_heart_segmentation_junzhe/segment_anything/modeling/sam.py:113\u001b[0m, in \u001b[0;36mSam.forward\u001b[0;34m(self, batched_input, multimask_output)\u001b[0m\n\u001b[1;32m    107\u001b[0m     points \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    108\u001b[0m sparse_embeddings, dense_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_encoder(\n\u001b[1;32m    109\u001b[0m     points\u001b[39m=\u001b[39mpoints,\n\u001b[1;32m    110\u001b[0m     boxes\u001b[39m=\u001b[39mimage_record\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    111\u001b[0m     masks\u001b[39m=\u001b[39mimage_record\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmask_inputs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    112\u001b[0m )\n\u001b[0;32m--> 113\u001b[0m low_res_masks, iou_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmask_decoder(\n\u001b[1;32m    114\u001b[0m     image_embeddings\u001b[39m=\u001b[39;49mcurr_embedding\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m),\n\u001b[1;32m    115\u001b[0m     image_pe\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt_encoder\u001b[39m.\u001b[39;49mget_dense_pe(), \n\u001b[1;32m    116\u001b[0m     sparse_prompt_embeddings\u001b[39m=\u001b[39;49msparse_embeddings,\n\u001b[1;32m    117\u001b[0m     dense_prompt_embeddings\u001b[39m=\u001b[39;49mdense_embeddings,\n\u001b[1;32m    118\u001b[0m     multimask_output\u001b[39m=\u001b[39;49mmultimask_output,\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    120\u001b[0m masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess_masks(\n\u001b[1;32m    121\u001b[0m     low_res_masks,\n\u001b[1;32m    122\u001b[0m     input_size\u001b[39m=\u001b[39m image_record[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:],\n\u001b[1;32m    123\u001b[0m     original_size\u001b[39m=\u001b[39m image_record[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:],\u001b[39m# image_record[\"original_size\"],\u001b[39;00m\n\u001b[1;32m    124\u001b[0m )\n\u001b[1;32m    125\u001b[0m \u001b[39m######masks = masks > self.mask_threshold\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/host/d/GitHub/Whole_heart_segmentation_junzhe/segment_anything/modeling/mask_decoder.py:105\u001b[0m, in \u001b[0;36mMaskDecoder.forward\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output)\u001b[0m\n\u001b[1;32m    102\u001b[0m hs, src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(image_embeddings, image_pe, sparse_prompt_embeddings)\n\u001b[1;32m    104\u001b[0m \u001b[39m# 分离出 iou_token 和 mask_tokens 的输出\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m iou_token_out \u001b[39m=\u001b[39m hs[:, \u001b[39m0\u001b[39;49m, :]\n\u001b[1;32m    106\u001b[0m mask_tokens_out \u001b[39m=\u001b[39m hs[:, \u001b[39m1\u001b[39m : (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_mask_tokens), :]\n\u001b[1;32m    108\u001b[0m \u001b[39m# 2. 上采样图像嵌入 (Upscale image embeddings)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m# src 形状从 [B, L, C] 转为 [B, C, H, W]\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "our_model.to(device)\n",
    "for batch_data in dl:\n",
    "    for key, value in batch_data.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            batch_data[key] = value.to(device)\n",
    "\n",
    "    # change batch_data[\"image\"] shape from (1, C, H, W) to (C,H,W)\n",
    "    batch_data[\"image\"] = batch_data[\"image\"].squeeze(0)\n",
    "\n",
    "    output = our_model([batch_data], multimask_output=True)\n",
    "    print('output masks shape:', output[0]['masks'].shape)\n",
    "\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ce1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][\"masks\"].shape  # (num_masks, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = output[0][\"masks\"].cpu().numpy()\n",
    "print('data maxx and minn:', np.max(a), np.min(a))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa676b",
   "metadata": {},
   "source": [
    "定义LOSS函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e967925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置优化器 \n",
    "trainable_params = filter(lambda p: p.requires_grad, our_model.parameters())\n",
    "optimizer = optim.AdamW(trainable_params, lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义 Loss\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#混合精度 Scaler\n",
    "loss_scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f13715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"models/my_trial\", exist_ok=True)\n",
    "print(\"文件夹创建成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29b32c",
   "metadata": {},
   "source": [
    "进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43538729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 强制手动定义轮数，不再依赖容易出错的 args\n",
    "# -------------------------------------------------------------\n",
    "RUN_EPOCHS = 50   # <--- 我们直接在这里定义，不用管 args 里叫什么了\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# 计算总步数\n",
    "total_steps = len(dl) * RUN_EPOCHS\n",
    "\n",
    "print(f\" 开始训练... 共 {RUN_EPOCHS} 轮，总计 {total_steps} 步\")\n",
    "\n",
    "our_model.train() # 切换到训练模式\n",
    "\n",
    "# 创建总进度条\n",
    "pbar = tqdm(total=total_steps, desc=\"总体训练进度\", unit=\"step\")\n",
    "\n",
    "for epoch in range(1, RUN_EPOCHS + 1):\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # 内层循环直接遍历 dl\n",
    "    for step, batch_data in enumerate(dl):\n",
    "        \n",
    "        # --- (A) 数据搬运 ---\n",
    "        images = batch_data[\"image\"].cuda()\n",
    "        gt_masks = batch_data[\"mask\"].cuda().long().squeeze(1)\n",
    "        \n",
    "        batched_input = []\n",
    "        for i in range(len(images)):\n",
    "            batched_input.append({\n",
    "                'image': images[i],\n",
    "                'original_size': (args.img_size, args.img_size)\n",
    "            })\n",
    "            \n",
    "        # --- (B) 前向传播 ---\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = our_model(batched_input, multimask_output=True)\n",
    "            pred_masks = torch.stack([o['masks'] for o in outputs]).squeeze(1)\n",
    "            loss = criterion(pred_masks, gt_masks)\n",
    "        \n",
    "        # --- (C) 反向传播 ---\n",
    "        loss_scaler.scale(loss).backward()\n",
    "        loss_scaler.step(optimizer)\n",
    "        loss_scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # 4. 手动更新总进度条\n",
    "        pbar.update(1) \n",
    "        pbar.set_postfix({\n",
    "            'epoch': f\"{epoch}/{RUN_EPOCHS}\",\n",
    "            'loss': f\"{loss.item():.4f}\"\n",
    "        })\n",
    "\n",
    "    # (可选) 打印每轮平均 Loss\n",
    "    # avg_loss = epoch_loss / len(dl)\n",
    "    # print(f\"Epoch {epoch} 结束 | Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # 保存模型\n",
    "    if epoch % 5 == 0:\n",
    "        save_path = os.path.join(\"models/my_trial\", f\"model_epoch_{epoch}.pth\")\n",
    "        torch.save(our_model.state_dict(), save_path)\n",
    "\n",
    "pbar.close()\n",
    "print(\"\\n 训练全部完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Whole_heart_segmentation_junzhe.functions_collection as ff\n",
    "import numpy as np\n",
    "\n",
    "# 1. 模拟一个心脏 Mask (在 128x128 空间中)\n",
    "test_mask = np.zeros((20, 128, 128))\n",
    "test_mask[5:15, 40:60, 50:80] = 1  # 这是一个长方形的心脏区域\n",
    "\n",
    "# 2. 调用函数\n",
    "bbox_info = ff.get_tilted_3d_bbox(test_mask, padding=5)\n",
    "\n",
    "# 3. 验证结果\n",
    "if bbox_info:\n",
    "    x1, y1, x2, y2 = bbox_info['sam_prompt']\n",
    "    print(f\"生成的 BBox: {bbox_info['sam_prompt']}\")\n",
    "    print(f\"X宽度: {x2 - x1}, Y宽度: {y2 - y1}\") # 应该相等！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
